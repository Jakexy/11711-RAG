The paper title is Improving Language Models with Advantage-based Offline Policy Gradients.
The faculty author of the paper Improving Language Models with Advantage-based Offline Policy Gradients is Maarten Sap.
The paper Improving Language Models with Advantage-based Offline Policy Gradients publication year is 2023.
Co-authors of the paper Improving Language Models with Advantage-based Offline Policy Gradients are Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark O. Riedl.
The publication ID of the paper Improving Language Models with Advantage-based Offline Policy Gradients is 1901e811-ee72-4b20-8f7e-de08cd395a10.
The publication name of the paper Improving Language Models with Advantage-based Offline Policy Gradients is arXiv.org.
Publication alternate names of the paper Improving Language Models with Advantage-based Offline Policy Gradients are ArXiv.
The publication issn of the paper Improving Language Models with Advantage-based Offline Policy Gradients is 2331-8422.
The publication url of the paper Improving Language Models with Advantage-based Offline Policy Gradients is https://arxiv.org.
The paper abstract is Language Models (LMs) achieve substantial language capabilities when finetuned using Reinforcement Learning with Human Feedback (RLHF). However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM's internal sequence-level value estimate, A-LoL filters negative advantage (low-quality) data points during training, making it resilient to noise. Overall, A-LoL is an easy-to-implement LM training recipe that is sample-efficient and stable. We demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data. We also release our experimental code. https://github.com/abaheti95/LoL-RL
