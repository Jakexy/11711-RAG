The paper title is Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding.
The faculty author of the paper Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding is Shinji Watanabe.
The paper Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding publication year is 2023.
Co-authors of the paper Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding are Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe.
The publication ID of the paper Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding is 0d6f7fba-7092-46b3-8039-93458dba736b.
The publication name of the paper Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding is IEEE International Conference on Acoustics, Speech, and Signal Processing.
The publication type of the paper Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding is conference.
Publication alternate names of the paper Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding are Int Conf Acoust Speech Signal Process, IEEE Int Conf Acoust Speech Signal Process, ICASSP, International Conference on Acoustics, Speech, and Signal Processing.
The publication url of the paper Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding is http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002.
The paper abstract is Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.
