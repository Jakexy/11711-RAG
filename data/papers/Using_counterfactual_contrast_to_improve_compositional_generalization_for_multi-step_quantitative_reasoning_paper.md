The paper title is Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning.
The faculty author of the paper Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning is Carolyn Rosé.
The paper Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning publication year is 2023.
Co-authors of the paper Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning are Armineh Nourbakhsh, Sameena Shah, C. Rosé.
The publication ID of the paper Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning is 1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44.
The publication name of the paper Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning is Annual Meeting of the Association for Computational Linguistics.
The publication type of the paper Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning is conference.
Publication alternate names of the paper Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning are Annu Meet Assoc Comput Linguistics, Meeting of the Association for Computational Linguistics, ACL, Meet Assoc Comput Linguistics.
The publication url of the paper Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning is https://www.aclweb.org/anthology/venues/acl/.
The paper abstract is In quantitative question answering, compositional generalization is one of the main challenges of state of the art models, especially when longer sequences of reasoning steps are required. In this paper we propose CounterComp, a method that uses counterfactual scenarios to generate samples with compositional contrast. Instead of a data augmentation approach, CounterComp is based on metric learning, which allows for direct sampling from the training set and circumvents the need for additional human labels. Our proposed auxiliary metric learning loss improves the performance of three state of the art models on four recently released datasets. We also show how the approach can improve OOD performance on unseen domains, as well as unseen compositions. Lastly, we demonstrate how the method can lead to better compositional attention patterns during training.
