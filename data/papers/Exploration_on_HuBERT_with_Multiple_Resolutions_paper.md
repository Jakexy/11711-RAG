Faculty: Shinji Watanabe
Title: Exploration on HuBERT with Multiple Resolutions
Abstract: Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.
Year: 2023
Authors: Jiatong Shi, Yun Tang, H. Inaguma, Hongyu Gong, J. Pino, Shinji Watanabe
Publication ID: af90489e-312f-4514-bea2-bcb399cb8ece
Publication Name: Interspeech
Publication Type: conference
Publication Alternate Names: Conf Int Speech Commun Assoc, INTERSPEECH, Conference of the International Speech Communication Association
Publication Issn: 2308-457X
Publication Url: https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech
Publication Alternate Urls: http://www.isca-speech.org/
