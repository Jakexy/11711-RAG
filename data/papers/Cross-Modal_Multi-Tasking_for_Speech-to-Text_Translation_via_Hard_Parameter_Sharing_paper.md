The paper title is Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing.
The faculty author of the paper Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing is Shinji Watanabe.
The paper Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing publication year is 2023.
Co-authors of the paper Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing are Brian Yan, Xuankai Chang, Antonios Anastasopoulos, Yuya Fujita, Shinji Watanabe.
The publication ID of the paper Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing is 1901e811-ee72-4b20-8f7e-de08cd395a10.
The publication name of the paper Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing is arXiv.org.
Publication alternate names of the paper Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing are ArXiv.
The publication issn of the paper Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing is 2331-8422.
The publication url of the paper Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing is https://arxiv.org.
The paper abstract is Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.
