The paper title is The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment.
The faculty author of the paper The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment is Yonatan Bisk.
The paper The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment publication year is 2023.
Co-authors of the paper The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment are Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell.
The publication ID of the paper The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment is 41bf9ed3-85b3-4c90-b015-150e31690253.
The publication name of the paper The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment is Conference on Empirical Methods in Natural Language Processing.
The publication type of the paper The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment is conference.
Publication alternate names of the paper The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment are Empir Method Nat Lang Process, Empirical Methods in Natural Language Processing, Conf Empir Method Nat Lang Process, EMNLP.
The publication url of the paper The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment is https://www.aclweb.org/portal/emnlp.
The paper abstract is Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.
