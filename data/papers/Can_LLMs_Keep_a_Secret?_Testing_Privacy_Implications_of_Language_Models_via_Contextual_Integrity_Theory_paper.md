The paper title is Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory.
The faculty author of the paper Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory is Maarten Sap.
The paper Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory publication year is 2023.
Co-authors of the paper Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory are Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, Yejin Choi.
The publication ID of the paper Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory is 1901e811-ee72-4b20-8f7e-de08cd395a10.
The publication name of the paper Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory is arXiv.org.
Publication alternate names of the paper Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory are ArXiv.
The publication issn of the paper Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory is 2331-8422.
The publication url of the paper Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory is https://arxiv.org.
The paper abstract is The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.
