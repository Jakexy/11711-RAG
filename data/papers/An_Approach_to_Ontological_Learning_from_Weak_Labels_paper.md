The paper title is An Approach to Ontological Learning from Weak Labels.
The faculty author of the paper An Approach to Ontological Learning from Weak Labels is Bhiksha Raj.
The paper An Approach to Ontological Learning from Weak Labels publication year is 2023.
Co-authors of the paper An Approach to Ontological Learning from Weak Labels are Ankit Shah, Larry Tang, Po Hao Chou, Yilun Zheng, Ziqian Ge, B. Raj.
The publication ID of the paper An Approach to Ontological Learning from Weak Labels is 0d6f7fba-7092-46b3-8039-93458dba736b.
The publication name of the paper An Approach to Ontological Learning from Weak Labels is IEEE International Conference on Acoustics, Speech, and Signal Processing.
The publication type of the paper An Approach to Ontological Learning from Weak Labels is conference.
Publication alternate names of the paper An Approach to Ontological Learning from Weak Labels are Int Conf Acoust Speech Signal Process, IEEE Int Conf Acoust Speech Signal Process, ICASSP, International Conference on Acoustics, Speech, and Signal Processing.
The publication url of the paper An Approach to Ontological Learning from Weak Labels is http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002.
The paper abstract is Ontologies encompass a formal representation of knowledge through the definition of concepts or properties of a domain, and the relationships between those concepts. In this work, we seek to investigate whether using this ontological information will improve learning from weakly labeled data, which are easier to collect since it requires only the presence or absence of an event to be known. We use the AudioSet ontology and dataset, which contains audio clips weakly labeled with the ontology concepts and the ontology providing the "Is A" relations between the concepts. We first re-implemented the model proposed by [1] with modifications to fit the multi-label scenario and then expand on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts. We find that the baseline Twin Neural Network (TNN) does not perform better by incorporating ontology information in the weak and multi-label scenario, but that the GCN does capture the ontology knowledge better for weak, multi-labeled data. We also investigate how different modules can tolerate noises introduced from weak labels and better incorporate ontology information. Our best TNN-GCN model achieves mAP=0.45 and AUC=0.87 for lower-level concepts and mAP=0.72 and AUC=0.86 for higher-level concepts, which is an improvement over the baseline TNN but about the same as our models that do not use ontology information.
