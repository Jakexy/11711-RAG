The paper title is Large Language Models Enable Few-Shot Clustering.
The faculty author of the paper Large Language Models Enable Few-Shot Clustering is Graham Neubig.
The paper Large Language Models Enable Few-Shot Clustering publication year is 2023.
Co-authors of the paper Large Language Models Enable Few-Shot Clustering are Vijay Viswanathan, Kiril Gashteovski, Carolin (Haas) Lawrence, Tongshuang Sherry Wu, Graham Neubig.
The publication ID of the paper Large Language Models Enable Few-Shot Clustering is 1901e811-ee72-4b20-8f7e-de08cd395a10.
The publication name of the paper Large Language Models Enable Few-Shot Clustering is arXiv.org.
Publication alternate names of the paper Large Language Models Enable Few-Shot Clustering are ArXiv.
The publication issn of the paper Large Language Models Enable Few-Shot Clustering is 2331-8422.
The publication url of the paper Large Language Models Enable Few-Shot Clustering is https://arxiv.org.
The paper abstract is Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user's intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model can amplify an expert's guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find incorporating LLMs in the first two stages can routinely provide significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.
