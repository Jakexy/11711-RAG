The paper title is Speaker-Independent Acoustic-to-Articulatory Speech Inversion.
The faculty author of the paper Speaker-Independent Acoustic-to-Articulatory Speech Inversion is Shinji Watanabe.
The paper Speaker-Independent Acoustic-to-Articulatory Speech Inversion publication year is 2023.
Co-authors of the paper Speaker-Independent Acoustic-to-Articulatory Speech Inversion are Peter Wu, Li-Wei Chen, Cheol Jun Cho, Shinji Watanabe, L. Goldstein, A. Black, G. Anumanchipalli.
The publication ID of the paper Speaker-Independent Acoustic-to-Articulatory Speech Inversion is 0d6f7fba-7092-46b3-8039-93458dba736b.
The publication name of the paper Speaker-Independent Acoustic-to-Articulatory Speech Inversion is IEEE International Conference on Acoustics, Speech, and Signal Processing.
The publication type of the paper Speaker-Independent Acoustic-to-Articulatory Speech Inversion is conference.
Publication alternate names of the paper Speaker-Independent Acoustic-to-Articulatory Speech Inversion are Int Conf Acoust Speech Signal Process, IEEE Int Conf Acoust Speech Signal Process, ICASSP, International Conference on Acoustics, Speech, and Signal Processing.
The publication url of the paper Speaker-Independent Acoustic-to-Articulatory Speech Inversion is http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002.
The paper abstract is To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promising inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulography (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these representations through directly com-paring the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset.
