The paper title is BiasX: "Thinking Slow" in Toxic Content Moderation with Explanations of Implied Social Biases.
The faculty author of the paper BiasX: "Thinking Slow" in Toxic Content Moderation with Explanations of Implied Social Biases is Maarten Sap.
The paper BiasX: "Thinking Slow" in Toxic Content Moderation with Explanations of Implied Social Biases publication year is 2023.
Co-authors of the paper BiasX: "Thinking Slow" in Toxic Content Moderation with Explanations of Implied Social Biases are Yiming Zhang, Sravani Nanduri, Liwei Jiang, Tongshuang Wu, Maarten Sap.
The publication ID of the paper BiasX: "Thinking Slow" in Toxic Content Moderation with Explanations of Implied Social Biases is 41bf9ed3-85b3-4c90-b015-150e31690253.
The publication name of the paper BiasX: "Thinking Slow" in Toxic Content Moderation with Explanations of Implied Social Biases is Conference on Empirical Methods in Natural Language Processing.
The publication type of the paper BiasX: "Thinking Slow" in Toxic Content Moderation with Explanations of Implied Social Biases is conference.
Publication alternate names of the paper BiasX: "Thinking Slow" in Toxic Content Moderation with Explanations of Implied Social Biases are Empir Method Nat Lang Process, Empirical Methods in Natural Language Processing, Conf Empir Method Nat Lang Process, EMNLP.
The publication url of the paper BiasX: "Thinking Slow" in Toxic Content Moderation with Explanations of Implied Social Biases is https://www.aclweb.org/portal/emnlp.
The paper abstract is Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. The quality of explanations is critical: imperfect machine-generated explanations (+2.4% on hard toxic examples) help less compared to expert-written human explanations (+7.2%). Our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.
