The paper title is Energy and Carbon Considerations of Fine-Tuning BERT.
The faculty author of the paper Energy and Carbon Considerations of Fine-Tuning BERT is Emma Strubell.
The paper Energy and Carbon Considerations of Fine-Tuning BERT publication year is 2023.
Co-authors of the paper Energy and Carbon Considerations of Fine-Tuning BERT are Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, Sasha Luccioni.
The publication ID of the paper Energy and Carbon Considerations of Fine-Tuning BERT is 41bf9ed3-85b3-4c90-b015-150e31690253.
The publication name of the paper Energy and Carbon Considerations of Fine-Tuning BERT is Conference on Empirical Methods in Natural Language Processing.
The publication type of the paper Energy and Carbon Considerations of Fine-Tuning BERT is conference.
Publication alternate names of the paper Energy and Carbon Considerations of Fine-Tuning BERT are Empir Method Nat Lang Process, Empirical Methods in Natural Language Processing, Conf Empir Method Nat Lang Process, EMNLP.
The publication url of the paper Energy and Carbon Considerations of Fine-Tuning BERT is https://www.aclweb.org/portal/emnlp.
The paper abstract is Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.
