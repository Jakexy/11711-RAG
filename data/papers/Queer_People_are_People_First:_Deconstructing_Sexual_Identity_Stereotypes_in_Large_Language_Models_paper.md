The paper title is Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models.
The faculty author of the paper Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models is Emma Strubell.
The paper Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models publication year is 2023.
Co-authors of the paper Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models are Harnoor Dhingra, Preetiha Jayashanker, Sayali S. Moghe, Emma Strubell.
The publication ID of the paper Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models is 1901e811-ee72-4b20-8f7e-de08cd395a10.
The publication name of the paper Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models is arXiv.org.
Publication alternate names of the paper Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models are ArXiv.
The publication issn of the paper Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models is 2331-8422.
The publication url of the paper Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models is https://arxiv.org.
The paper abstract is Large Language Models (LLMs) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs generate text describing people with different sexual identities. Analyzing bias in the text generated by an LLM using regard score shows measurable bias against queer people. We then show that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.
