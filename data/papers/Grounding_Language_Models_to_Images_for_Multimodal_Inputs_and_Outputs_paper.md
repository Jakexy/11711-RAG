The paper title is Grounding Language Models to Images for Multimodal Inputs and Outputs.
The faculty author of the paper Grounding Language Models to Images for Multimodal Inputs and Outputs is Daniel Fried.
The paper Grounding Language Models to Images for Multimodal Inputs and Outputs publication year is 2023.
Co-authors of the paper Grounding Language Models to Images for Multimodal Inputs and Outputs are Jing Yu Koh, R. Salakhutdinov, Daniel Fried.
The publication ID of the paper Grounding Language Models to Images for Multimodal Inputs and Outputs is fc0a208c-acb7-47dc-a0d4-af8190e21d29.
The publication name of the paper Grounding Language Models to Images for Multimodal Inputs and Outputs is International Conference on Machine Learning.
The publication type of the paper Grounding Language Models to Images for Multimodal Inputs and Outputs is conference.
Publication alternate names of the paper Grounding Language Models to Images for Multimodal Inputs and Outputs are ICML, Int Conf Mach Learn.
The publication url of the paper Grounding Language Models to Images for Multimodal Inputs and Outputs is https://icml.cc/.
The paper abstract is We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
