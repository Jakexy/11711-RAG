The paper title is The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation.
The faculty author of the paper The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation is Graham Neubig.
The paper The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation publication year is 2023.
Co-authors of the paper The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation are Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, Andr√© F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat.
The publication ID of the paper The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation is 9aacb914-3edf-4e02-b8fe-5abf21c4d2ba.
The publication name of the paper The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation is Conference on Machine Translation.
The publication type of the paper The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation is conference.
Publication alternate names of the paper The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation are WMT, Conf Mach Transl.
The paper abstract is Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.
