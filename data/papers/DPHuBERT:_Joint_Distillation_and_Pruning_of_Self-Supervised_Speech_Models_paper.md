The paper title is DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models.
The faculty author of the paper DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models is Shinji Watanabe.
The paper DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models publication year is 2023.
Co-authors of the paper DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models are Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe.
The publication ID of the paper DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models is af90489e-312f-4514-bea2-bcb399cb8ece.
The publication name of the paper DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models is Interspeech.
The publication type of the paper DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models is conference.
Publication alternate names of the paper DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models are Conf Int Speech Commun Assoc, INTERSPEECH, Conference of the International Speech Communication Association.
The publication issn of the paper DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models is 2308-457X.
The publication url of the paper DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models is https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech.
Publication alternate urls of the paper DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models are http://www.isca-speech.org/.
The paper abstract is Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available.
