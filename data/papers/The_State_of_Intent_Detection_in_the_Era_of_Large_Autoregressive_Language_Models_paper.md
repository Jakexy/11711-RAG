The paper title is The State of Intent Detection in the Era of Large Autoregressive Language Models.
The faculty author of the paper The State of Intent Detection in the Era of Large Autoregressive Language Models is Daphne Ippolito.
The paper The State of Intent Detection in the Era of Large Autoregressive Language Models publication year is 2023.
Co-authors of the paper The State of Intent Detection in the Era of Large Autoregressive Language Models are Tom B. Brown, Benjamin Mann, Nick Ryder, Jared D Subbiah, Prafulla Kaplan, A. Dhariwal, P. Neelakantan, Girish Shyam, Amanda Sastry, Sandhini Askell, Ariel Agarwal, Herbert-Voss, Gretchen Krueger, T. Henighan, R. Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Christopher Winter, Mark Hesse, Eric Chen, Mateusz Sigler, Scott teusz Litwin, Benjamin Gray, Jack Chess, Christopher Clark, Sam Berner, Alec McCandlish, Ilya Radford, Sutskever Dario, Amodei, Matthew Henderson, Ivan Vulic. 2020, Ef-310, Aakanksha Chowdhery, Sharan Narang, J. Devlin, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek, Parker Rao, Yi Barnes, Noam Tay, Vin-316 Shazeer, Emily odkumar Prabhakaran, Nan Reif, Ben Du, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, H. Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, L. Fedus, Denny Zhou, Daphne Ippolito, D. Luan, Hyeontaek Lim, Barret Zoph, A. Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Oleksandr Polozov, K. Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta.
The paper abstract is In-context learning (ICL) using large pre-001 trained autoregressive language models (LLMs, 002 e.g. GPT-3) has demonstrated effective clas-003 sification performance at a variety of natural 004 language tasks. Using LLMs for intent detec-005 tion is challenging due to the large label space 006 and limited context window, such that it is diffi-007 cult to fit a sufficient number of examples in the 008 prompt to allow the use of in-context learning. 009 In this paper, dense retrieval is used to bypass 010 this limitation, giving the model only a par-011 tial view of the full label space. We show that 012 retriever-augmented large language models are 013 an effective way to tackle intent detection, by-014 passing context window limitations effectively 015 through the retrieval mechanism. Comparing 016 the LLaMA and OPT model families at differ-017 ent scales, we set new state of the art perfor-018 mance in the few-shot setting with zero training 019 for two of the three intent classification datasets 020 that we consider, while achieving competitive 021 results on the third one. This work demon-022 strates that the Retriever+ICL framework is a 023 strong zero-training competitor to fine-tuned in-024 tent detection approaches. In addition, a small 025 study on the number of examples provided at 026 different model scales is done, showing that 027 larger models are needed to make effective use 028 of more examples in-prompt. 029
