The paper title is Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation.
The faculty author of the paper Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation is Graham Neubig.
The paper Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation publication year is 2023.
Co-authors of the paper Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation are Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Sherry Wu, Graham Neubig, André F. T. Martins.
The publication ID of the paper Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation is 1901e811-ee72-4b20-8f7e-de08cd395a10.
The publication name of the paper Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation is arXiv.org.
Publication alternate names of the paper Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation are ArXiv.
The publication issn of the paper Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation is 2331-8422.
The publication url of the paper Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation is https://arxiv.org.
The paper abstract is Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.
