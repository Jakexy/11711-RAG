The paper title is Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation.
The faculty author of the paper Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation is Yiming Yang.
The paper Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation publication year is 2023.
Co-authors of the paper Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation are Junwei Huang, Zhiqing Sun, Yiming Yang.
The publication ID of the paper Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation is 1901e811-ee72-4b20-8f7e-de08cd395a10.
The publication name of the paper Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation is arXiv.org.
Publication alternate names of the paper Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation are ArXiv.
The publication issn of the paper Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation is 2331-8422.
The publication url of the paper Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation is https://arxiv.org.
The paper abstract is Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.
