The paper title is Grounding Language Models to Images for Multimodal Generation.
The faculty author of the paper Grounding Language Models to Images for Multimodal Generation is Daniel Fried.
The paper Grounding Language Models to Images for Multimodal Generation publication year is 2023.
Co-authors of the paper Grounding Language Models to Images for Multimodal Generation are Jing Yu Koh, R. Salakhutdinov, Daniel Fried.
The publication ID of the paper Grounding Language Models to Images for Multimodal Generation is 1901e811-ee72-4b20-8f7e-de08cd395a10.
The publication name of the paper Grounding Language Models to Images for Multimodal Generation is arXiv.org.
Publication alternate names of the paper Grounding Language Models to Images for Multimodal Generation are ArXiv.
The publication issn of the paper Grounding Language Models to Images for Multimodal Generation is 2331-8422.
The publication url of the paper Grounding Language Models to Images for Multimodal Generation is https://arxiv.org.
The paper abstract is We propose an efﬁcient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and ﬁnetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text inter-leaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
