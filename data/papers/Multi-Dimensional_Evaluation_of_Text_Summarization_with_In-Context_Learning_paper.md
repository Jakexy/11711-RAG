The paper title is Multi-Dimensional Evaluation of Text Summarization with In-Context Learning.
The faculty author of the paper Multi-Dimensional Evaluation of Text Summarization with In-Context Learning is Graham Neubig.
The paper Multi-Dimensional Evaluation of Text Summarization with In-Context Learning publication year is 2023.
Co-authors of the paper Multi-Dimensional Evaluation of Text Summarization with In-Context Learning are Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, Chunting Zhou.
The publication ID of the paper Multi-Dimensional Evaluation of Text Summarization with In-Context Learning is 1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44.
The publication name of the paper Multi-Dimensional Evaluation of Text Summarization with In-Context Learning is Annual Meeting of the Association for Computational Linguistics.
The publication type of the paper Multi-Dimensional Evaluation of Text Summarization with In-Context Learning is conference.
Publication alternate names of the paper Multi-Dimensional Evaluation of Text Summarization with In-Context Learning are Annu Meet Assoc Comput Linguistics, Meeting of the Association for Computational Linguistics, ACL, Meet Assoc Comput Linguistics.
The publication url of the paper Multi-Dimensional Evaluation of Text Summarization with In-Context Learning is https://www.aclweb.org/anthology/venues/acl/.
The paper abstract is Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.
