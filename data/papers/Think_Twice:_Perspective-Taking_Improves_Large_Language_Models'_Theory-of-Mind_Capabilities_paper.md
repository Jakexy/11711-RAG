The paper title is Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities.
The faculty author of the paper Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities is Louis-Philippe Morency.
The paper Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities publication year is 2023.
Co-authors of the paper Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities are Alex Wilf, Sihyun Shawn Lee, P. Liang, Louis-Philippe Morency.
The publication ID of the paper Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities is 1901e811-ee72-4b20-8f7e-de08cd395a10.
The publication name of the paper Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities is arXiv.org.
Publication alternate names of the paper Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities are ArXiv.
The publication issn of the paper Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities is 2331-8422.
The publication url of the paper Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities is https://arxiv.org.
The paper abstract is Human interactions are deeply rooted in the interplay of thoughts, beliefs, and desires made possible by Theory of Mind (ToM): our cognitive ability to understand the mental states of ourselves and others. Although ToM may come naturally to us, emulating it presents a challenge to even the most advanced Large Language Models (LLMs). Recent improvements to LLMs' reasoning capabilities from simple yet effective prompting techniques such as Chain-of-Thought have seen limited applicability to ToM. In this paper, we turn to the prominent cognitive science theory"Simulation Theory"to bridge this gap. We introduce SimToM, a novel two-stage prompting framework inspired by Simulation Theory's notion of perspective-taking. To implement this idea on current ToM benchmarks, SimToM first filters context based on what the character in question knows before answering a question about their mental state. Our approach, which requires no additional training and minimal prompt-tuning, shows substantial improvement over existing methods, and our analysis reveals the importance of perspective-taking to Theory-of-Mind capabilities. Our findings suggest perspective-taking as a promising direction for future research into improving LLMs' ToM capabilities.
