The paper title is Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study.
The faculty author of the paper Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study is Shinji Watanabe.
The paper Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study publication year is 2023.
Co-authors of the paper Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study are Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma, Jiatong Shi, Jinchuan Tian, Shinji Watanabe, Yuya Fujita, Takashi Maekaku, Pengcheng Guo, Yao-Fei Cheng, Pavel Denisov, Kohei Saijo, Hsiu-Hsuan Wang.
The publication ID of the paper Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study is 1901e811-ee72-4b20-8f7e-de08cd395a10.
The publication name of the paper Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study is arXiv.org.
Publication alternate names of the paper Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study are ArXiv.
The publication issn of the paper Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study is 2331-8422.
The publication url of the paper Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study is https://arxiv.org.
The paper abstract is Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.
