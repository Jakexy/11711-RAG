The paper title is AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models.
The faculty author of the paper AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models is Shinji Watanabe.
The paper AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models publication year is 2023.
Co-authors of the paper AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models are Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David F. Harwath, Yu Tsao, Shinji Watanabe, Abdel-rahman Mohamed, Chi-Luen Feng, Hung-yi Lee.
The publication ID of the paper AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models is 1901e811-ee72-4b20-8f7e-de08cd395a10.
The publication name of the paper AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models is arXiv.org.
Publication alternate names of the paper AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models are ArXiv.
The publication issn of the paper AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models is 2331-8422.
The publication url of the paper AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models is https://arxiv.org.
The paper abstract is Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned representations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal audio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong intermediate task. We release our benchmark with evaluation code and a model submission platform to encourage further research in audio-visual learning.
