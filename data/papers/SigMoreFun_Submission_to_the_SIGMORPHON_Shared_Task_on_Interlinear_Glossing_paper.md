The paper title is SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing.
The faculty author of the paper SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing is Graham Neubig.
The paper SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing publication year is 2023.
Co-authors of the paper SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing are Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin.
The publication ID of the paper SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing is 56b19d19-ddf9-4e56-ba1e-37ef38ef5943.
The publication name of the paper SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing is Special Interest Group on Computational Morphology and Phonology Workshop.
The publication type of the paper SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing is conference.
Publication alternate names of the paper SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing are SIGMORPHON, Sp√©c Interest Group Comput Morphol Phonol Workshop.
The publication url of the paper SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing is http://sigmorphon.org/.
The paper abstract is In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.
