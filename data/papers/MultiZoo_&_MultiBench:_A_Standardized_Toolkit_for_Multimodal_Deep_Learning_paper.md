The paper title is MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning.
The faculty author of the paper MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning is Louis-Philippe Morency.
The paper MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning publication year is 2023.
Co-authors of the paper MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning are P. Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, R. Salakhutdinov.
The publication ID of the paper MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning is 1901e811-ee72-4b20-8f7e-de08cd395a10.
The publication name of the paper MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning is arXiv.org.
Publication alternate names of the paper MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning are ArXiv.
The publication issn of the paper MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning is 2331-8422.
The publication url of the paper MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning is https://arxiv.org.
The paper abstract is Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community.
