The paper title is Comparative Knowledge Distillation.
The faculty author of the paper Comparative Knowledge Distillation is Louis-Philippe Morency.
The paper Comparative Knowledge Distillation publication year is 2023.
Co-authors of the paper Comparative Knowledge Distillation are Alex Wilf, Alex Tianyi Xu, P. Liang, A. Obolenskiy, Daniel Fried, Louis-Philippe Morency.
The publication ID of the paper Comparative Knowledge Distillation is 1901e811-ee72-4b20-8f7e-de08cd395a10.
The publication name of the paper Comparative Knowledge Distillation is arXiv.org.
Publication alternate names of the paper Comparative Knowledge Distillation are ArXiv.
The publication issn of the paper Comparative Knowledge Distillation is 2331-8422.
The publication url of the paper Comparative Knowledge Distillation is https://arxiv.org.
The paper abstract is In the era of large scale pretrained models, Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally heavy teacher models to lightweight, efficient student models while preserving performance. Traditional KD paradigms, however, assume readily available access to teacher models for frequent inference -- a notion increasingly at odds with the realities of costly, often proprietary, large scale models. Addressing this gap, our paper considers how to minimize the dependency on teacher model inferences in KD in a setting we term Few Teacher Inference Knowledge Distillation (FTI KD). We observe that prevalent KD techniques and state of the art data augmentation strategies fall short in this constrained setting. Drawing inspiration from educational principles that emphasize learning through comparison, we propose Comparative Knowledge Distillation (CKD), which encourages student models to understand the nuanced differences in a teacher model's interpretations of samples. Critically, CKD provides additional learning signals to the student without making additional teacher calls. We also extend the principle of CKD to groups of samples, enabling even more efficient learning from limited teacher calls. Empirical evaluation across varied experimental settings indicates that CKD consistently outperforms state of the art data augmentation and KD techniques.
