The paper title is Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents.
The faculty author of the paper Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents is Yonatan Bisk.
The paper Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents publication year is 2023.
Co-authors of the paper Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents are Yue Wu, So Yeon Min, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Yuan-Fang Li, Tom M. Mitchell, Shrimai Prabhumoye.
The publication ID of the paper Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents is 1901e811-ee72-4b20-8f7e-de08cd395a10.
The publication name of the paper Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents is arXiv.org.
Publication alternate names of the paper Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents are ArXiv.
The publication issn of the paper Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents is 2331-8422.
The publication url of the paper Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents is https://arxiv.org.
The paper abstract is Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.
