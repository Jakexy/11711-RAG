The paper title is A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks.
The faculty author of the paper A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks is Shinji Watanabe.
The paper A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks publication year is 2023.
Co-authors of the paper A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks are Yifan Peng, Kwangyoun Kim, Felix Wu, Brian Yan, Siddhant Arora, William Chen, Jiyang Tang, Suwon Shon, Prashant Sridhar, Shinji Watanabe.
The publication ID of the paper A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks is af90489e-312f-4514-bea2-bcb399cb8ece.
The publication name of the paper A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks is Interspeech.
The publication type of the paper A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks is conference.
Publication alternate names of the paper A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks are Conf Int Speech Commun Assoc, INTERSPEECH, Conference of the International Speech Communication Association.
The publication issn of the paper A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks is 2308-457X.
The publication url of the paper A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks is https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech.
Publication alternate urls of the paper A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks are http://www.isca-speech.org/.
The paper abstract is Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.
