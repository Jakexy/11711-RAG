The paper title is Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications.
The faculty author of the paper Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications is Louis-Philippe Morency.
The paper Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications publication year is 2023.
Co-authors of the paper Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications are P. Liang, Chun Kai Ling, Yun Cheng, A. Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, R. Salakhutdinov.
The publication ID of the paper Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications is 1901e811-ee72-4b20-8f7e-de08cd395a10.
The publication name of the paper Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications is arXiv.org.
Publication alternate names of the paper Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications are ArXiv.
The publication issn of the paper Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications is 2331-8422.
The publication url of the paper Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications is https://arxiv.org.
The paper abstract is In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: the emergence of new task-relevant information during learning from both modalities that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contributions are the derivations of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms for min-entropy couplings. We validate these estimated bounds and show how they accurately track true interactions. Finally, two semi-supervised multimodal applications are explored based on these theoretical results: (1) analyzing the relationship between multimodal performance and estimated interactions, and (2) self-supervised learning that embraces disagreement between modalities beyond agreement as is typically done.
