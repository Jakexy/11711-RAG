The paper title is Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training.
The faculty author of the paper Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training is Emma Strubell.
The paper Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training publication year is 2023.
Co-authors of the paper Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training are Zhisong Zhang, Emma Strubell, E. Hovy.
The publication ID of the paper Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training is 41bf9ed3-85b3-4c90-b015-150e31690253.
The publication name of the paper Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training is Conference on Empirical Methods in Natural Language Processing.
The publication type of the paper Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training is conference.
Publication alternate names of the paper Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training are Empir Method Nat Lang Process, Empirical Methods in Natural Language Processing, Conf Empir Method Nat Lang Process, EMNLP.
The publication url of the paper Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training is https://www.aclweb.org/portal/emnlp.
The paper abstract is In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration.
