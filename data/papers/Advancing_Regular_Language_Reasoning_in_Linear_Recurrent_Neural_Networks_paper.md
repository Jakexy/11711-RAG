The paper title is Advancing Regular Language Reasoning in Linear Recurrent Neural Networks.
The faculty author of the paper Advancing Regular Language Reasoning in Linear Recurrent Neural Networks is Alexander Rudnicky.
The paper Advancing Regular Language Reasoning in Linear Recurrent Neural Networks publication year is 2023.
Co-authors of the paper Advancing Regular Language Reasoning in Linear Recurrent Neural Networks are Ting-Han Fan, Ta-Chung Chi, Alexander I. Rudnicky.
The publication ID of the paper Advancing Regular Language Reasoning in Linear Recurrent Neural Networks is 1901e811-ee72-4b20-8f7e-de08cd395a10.
The publication name of the paper Advancing Regular Language Reasoning in Linear Recurrent Neural Networks is arXiv.org.
Publication alternate names of the paper Advancing Regular Language Reasoning in Linear Recurrent Neural Networks are ArXiv.
The publication issn of the paper Advancing Regular Language Reasoning in Linear Recurrent Neural Networks is 2331-8422.
The publication url of the paper Advancing Regular Language Reasoning in Linear Recurrent Neural Networks is https://arxiv.org.
The paper abstract is In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language modeling and long-range modeling while offering rapid parallel training and constant inference costs. With the resurged interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations on regular language. Motivated by the analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.
