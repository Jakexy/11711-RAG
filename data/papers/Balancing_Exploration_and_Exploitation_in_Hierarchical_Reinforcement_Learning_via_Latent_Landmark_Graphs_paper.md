The paper title is Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs.
The faculty author of the paper Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs is Yiming Yang.
The paper Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs publication year is 2023.
Co-authors of the paper Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs are Qingyang Zhang, Yiming Yang, Jingqing Ruan, Xuantang Xiong, Dengpeng Xing, Bo Xu.
The publication ID of the paper Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs is f80ba4a3-7aed-4021-b4d8-e4f50668847a.
The publication name of the paper Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs is IEEE International Joint Conference on Neural Network.
The publication type of the paper Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs is conference.
Publication alternate names of the paper Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs are IJCNN, IEEE Int Jt Conf Neural Netw, Int Jt Conf Neural Netw, International Joint Conference on Neural Network.
The publication url of the paper Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs is http://www.wikicfp.com/cfp/program?id=1573.
The paper abstract is Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) is a promising paradigm to address the exploration-exploitation dilemma in reinforcement learning. It decomposes the source task into sub goal conditional subtasks and conducts exploration and exploitation in the subgoal space. The effectiveness of GCHRL heavily relies on sub goal representation functions and sub goal selection strategy. However, existing works often overlook the temporal coherence in GCHRL when learning latent sub goal representations and lack an efficient sub goal selection strategy that balances exploration and exploitation. This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome these limitations. HILL learns latent subgoal representations that satisfy temporal coherence using a contrastive representation learning objective. Based on these representations, HILL dynamically builds latent landmark graphs and employs a novelty measure on nodes and a utility measure on edges. Finally, HILL develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures. Experimental results demonstrate that HILL outperforms state-of-the-art baselines on continuous control tasks with sparse rewards in sample efficiency and asymptotic performance. Our code is available at https://github.com/papercode2022/HILL.
