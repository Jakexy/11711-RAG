The paper title is SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning.
The faculty author of the paper SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning is Bhiksha Raj.
The paper SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning publication year is 2023.
Co-authors of the paper SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning are Hao Chen, R. Tao, Yue Fan, Yidong Wang, Jindong Wang, B. Schiele, Xingxu Xie, B. Raj, M. Savvides.
The publication ID of the paper SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning is 939c6e1d-0d17-4d6e-8a82-66d960df0e40.
The publication name of the paper SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning is International Conference on Learning Representations.
The publication type of the paper SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning is conference.
Publication alternate names of the paper SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning are Int Conf Learn Represent, ICLR.
The publication url of the paper SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning is https://iclr.cc/.
The paper abstract is The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and imbalanced classification.
