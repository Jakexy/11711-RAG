The paper title is Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning.
The faculty author of the paper Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning is Shinji Watanabe.
The paper Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning publication year is 2023.
Co-authors of the paper Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning are William Chen, Jiatong Shi, Brian Yan, Dan Berrebbi, Wangyou Zhang, Yifan Peng, Xuankai Chang, Soumi Maiti, Shinji Watanabe.
The publication ID of the paper Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning is 29014a7c-861f-43bd-b4d6-63edf4cd57ef.
The publication name of the paper Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning is Automatic Speech Recognition & Understanding.
The publication type of the paper Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning is conference.
Publication alternate names of the paper Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning are IEEE Automatic Speech Recognition and Understanding Workshop, Autom Speech Recognit  Underst, ASRU, IEEE Autom Speech Recognit Underst Workshop.
The paper abstract is Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This further harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We propose WavLabLM, which extends WavLM’s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than $10 \%$ of the training data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain $94 \%$ of XLS-R’s performance with only $3 \%$ of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet.
