The paper title is The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge.
The faculty author of the paper The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge is Shinji Watanabe.
The paper The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge publication year is 2023.
Co-authors of the paper The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge are Hayato Futami, Jessica Huynh, Siddhant Arora, Shih-Lun Wu, Yosuke Kashiwagi, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe.
The publication ID of the paper The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge is 0d6f7fba-7092-46b3-8039-93458dba736b.
The publication name of the paper The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge is IEEE International Conference on Acoustics, Speech, and Signal Processing.
The publication type of the paper The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge is conference.
Publication alternate names of the paper The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge are Int Conf Acoust Speech Signal Process, IEEE Int Conf Acoust Speech Signal Process, ICASSP, International Conference on Acoustics, Speech, and Signal Processing.
The publication url of the paper The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge is http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002.
The paper abstract is This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.
