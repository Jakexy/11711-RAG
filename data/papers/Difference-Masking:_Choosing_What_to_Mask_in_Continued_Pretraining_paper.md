The paper title is Difference-Masking: Choosing What to Mask in Continued Pretraining.
The faculty author of the paper Difference-Masking: Choosing What to Mask in Continued Pretraining is Louis-Philippe Morency.
The paper Difference-Masking: Choosing What to Mask in Continued Pretraining publication year is 2023.
Co-authors of the paper Difference-Masking: Choosing What to Mask in Continued Pretraining are Alex Wilf, Syeda Nahida Akter, Leena Mathur, P. Liang, Sheryl Mathew, Mengrou Shou, Eric Nyberg, Louis-Philippe Morency.
The publication ID of the paper Difference-Masking: Choosing What to Mask in Continued Pretraining is 41bf9ed3-85b3-4c90-b015-150e31690253.
The publication name of the paper Difference-Masking: Choosing What to Mask in Continued Pretraining is Conference on Empirical Methods in Natural Language Processing.
The publication type of the paper Difference-Masking: Choosing What to Mask in Continued Pretraining is conference.
Publication alternate names of the paper Difference-Masking: Choosing What to Mask in Continued Pretraining are Empir Method Nat Lang Process, Empirical Methods in Natural Language Processing, Conf Empir Method Nat Lang Process, EMNLP.
The publication url of the paper Difference-Masking: Choosing What to Mask in Continued Pretraining is https://www.aclweb.org/portal/emnlp.
The paper abstract is The self-supervised objective of masking-and-predicting has led to promising performance gains on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition that deciding what to mask can substantially improve learning outcomes. We investigate this in continued pretraining setting in which pretrained models continue to pretrain on domain-specific data before performing some downstream task. We introduce Difference-Masking, a masking strategy that automatically chooses what to mask during continued pretraining by considering what makes a task domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language-only and multimodal video tasks.
