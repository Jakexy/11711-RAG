The paper title is Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation.
The faculty author of the paper Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation is Shinji Watanabe.
The paper Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation publication year is 2023.
Co-authors of the paper Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation are Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhongqiu Wang, Nobutaka Ono, Y. Qian, Shinji Watanabe.
The publication ID of the paper Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation is 0bf449f6-ac85-4357-b5a8-0d5dc353c203.
The publication name of the paper Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation is IEEE Workshop on Applications of Signal Processing to Audio and Acoustics.
The publication type of the paper Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation is conference.
Publication alternate names of the paper Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation are IEEE Workshop Appl Signal Process Audio Acoust, Workshop on Applications of Signal Processing to Audio and Acoustics, Workshop Appl Signal Process Audio Acoust, WASPAA.
The publication url of the paper Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation is http://www.wikicfp.com/cfp/program?id=3007.
The paper abstract is Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).
