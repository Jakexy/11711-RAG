The paper title is Tensor decomposition for minimization of E2E SLU model toward on-device processing.
The faculty author of the paper Tensor decomposition for minimization of E2E SLU model toward on-device processing is Shinji Watanabe.
The paper Tensor decomposition for minimization of E2E SLU model toward on-device processing publication year is 2023.
Co-authors of the paper Tensor decomposition for minimization of E2E SLU model toward on-device processing are Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe.
The publication ID of the paper Tensor decomposition for minimization of E2E SLU model toward on-device processing is af90489e-312f-4514-bea2-bcb399cb8ece.
The publication name of the paper Tensor decomposition for minimization of E2E SLU model toward on-device processing is Interspeech.
The publication type of the paper Tensor decomposition for minimization of E2E SLU model toward on-device processing is conference.
Publication alternate names of the paper Tensor decomposition for minimization of E2E SLU model toward on-device processing are Conf Int Speech Commun Assoc, INTERSPEECH, Conference of the International Speech Communication Association.
The publication issn of the paper Tensor decomposition for minimization of E2E SLU model toward on-device processing is 2308-457X.
The publication url of the paper Tensor decomposition for minimization of E2E SLU model toward on-device processing is https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech.
Publication alternate urls of the paper Tensor decomposition for minimization of E2E SLU model toward on-device processing are http://www.isca-speech.org/.
The paper abstract is Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency property, unlike a cascade system, and aims to minimize the computational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architectures used in our E2E SLU models. We propose to apply singular value decomposition to linear layers and the Tucker decomposition to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposition to the Tucker decomposition. Since the E2E model is represented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 million parameters.
