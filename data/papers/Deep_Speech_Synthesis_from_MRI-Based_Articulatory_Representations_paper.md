The paper title is Deep Speech Synthesis from MRI-Based Articulatory Representations.
The faculty author of the paper Deep Speech Synthesis from MRI-Based Articulatory Representations is Shinji Watanabe.
The paper Deep Speech Synthesis from MRI-Based Articulatory Representations publication year is 2023.
Co-authors of the paper Deep Speech Synthesis from MRI-Based Articulatory Representations are Peter Wu, Tingle Li, Yijingxiu Lu, Yubin Zhang, Jiachen Lian, A. Black, L. Goldstein, Shinji Watanabe, G. Anumanchipalli.
The publication ID of the paper Deep Speech Synthesis from MRI-Based Articulatory Representations is af90489e-312f-4514-bea2-bcb399cb8ece.
The publication name of the paper Deep Speech Synthesis from MRI-Based Articulatory Representations is Interspeech.
The publication type of the paper Deep Speech Synthesis from MRI-Based Articulatory Representations is conference.
Publication alternate names of the paper Deep Speech Synthesis from MRI-Based Articulatory Representations are Conf Int Speech Commun Assoc, INTERSPEECH, Conference of the International Speech Communication Association.
The publication issn of the paper Deep Speech Synthesis from MRI-Based Articulatory Representations is 2308-457X.
The publication url of the paper Deep Speech Synthesis from MRI-Based Articulatory Representations is https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech.
Publication alternate urls of the paper Deep Speech Synthesis from MRI-Based Articulatory Representations are http://www.isca-speech.org/.
The paper abstract is In this paper, we study articulatory synthesis, a speech synthesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable synthesizers. While recent advances have enabled intelligible articulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excitation and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to enhance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Finally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and identify the most suitable MRI feature subset for articulatory synthesis.
