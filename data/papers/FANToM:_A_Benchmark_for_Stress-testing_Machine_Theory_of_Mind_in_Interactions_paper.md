The paper title is FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions.
The faculty author of the paper FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions is Maarten Sap.
The paper FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions publication year is 2023.
Co-authors of the paper FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions are Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, R. L. Bras, Gunhee Kim, Yejin Choi, Maarten Sap.
The publication ID of the paper FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions is 41bf9ed3-85b3-4c90-b015-150e31690253.
The publication name of the paper FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions is Conference on Empirical Methods in Natural Language Processing.
The publication type of the paper FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions is conference.
Publication alternate names of the paper FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions are Empir Method Nat Lang Process, Empirical Methods in Natural Language Processing, Conf Empir Method Nat Lang Process, EMNLP.
The publication url of the paper FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions is https://www.aclweb.org/portal/emnlp.
The paper abstract is Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.
