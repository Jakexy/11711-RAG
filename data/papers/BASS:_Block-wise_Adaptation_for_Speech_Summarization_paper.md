The paper title is BASS: Block-wise Adaptation for Speech Summarization.
The faculty author of the paper BASS: Block-wise Adaptation for Speech Summarization is Shinji Watanabe.
The paper BASS: Block-wise Adaptation for Speech Summarization publication year is 2023.
Co-authors of the paper BASS: Block-wise Adaptation for Speech Summarization are Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj.
The publication ID of the paper BASS: Block-wise Adaptation for Speech Summarization is af90489e-312f-4514-bea2-bcb399cb8ece.
The publication name of the paper BASS: Block-wise Adaptation for Speech Summarization is Interspeech.
The publication type of the paper BASS: Block-wise Adaptation for Speech Summarization is conference.
Publication alternate names of the paper BASS: Block-wise Adaptation for Speech Summarization are Conf Int Speech Commun Assoc, INTERSPEECH, Conference of the International Speech Communication Association.
The publication issn of the paper BASS: Block-wise Adaptation for Speech Summarization is 2308-457X.
The publication url of the paper BASS: Block-wise Adaptation for Speech Summarization is https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech.
Publication alternate urls of the paper BASS: Block-wise Adaptation for Speech Summarization are http://www.isca-speech.org/.
The paper abstract is End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.
